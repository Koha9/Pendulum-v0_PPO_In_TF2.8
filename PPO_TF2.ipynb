{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EP = 1000\n",
    "EP_LENGTH = 400\n",
    "GAMMA = 0.99 # discount future reward (UP?)\n",
    "EPSILON = 0.2 # clip Ratio range[1-EPSILON,1+EPSILON]\n",
    "ACTOR_LR = 1e-5 # LR\n",
    "CRITIC_LR = 2e-5 # LR\n",
    "BATCH = 32 # learning step\n",
    "ACTOR_EPOCH = 10 # epoch\n",
    "CRITIC_EPOCH = 10 # epoch\n",
    "ENTROPY_WHEIGHT = 0.01 # sigma's entropy in Actor loss\n",
    "ACTION_INTERVAL = 1 # take action every ACTION_INTERVAL steps\n",
    "TRAIN = True\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "SampleState = env.reset()\n",
    "\n",
    "STATE_SIZE = len(SampleState)\n",
    "ACTION_SIZE = 1\n",
    "\n",
    "CTN_ACTION_RANGE = 2\n",
    "print(\"state Size = \",STATE_SIZE)\n",
    "print(\"state Sample = \",SampleState)\n",
    "print(\"CTN_ACTION_RANGE = \",CTN_ACTION_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class buffer(object):\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    def clearBuffer(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "    def getStates(self):\n",
    "        return self.states\n",
    "    def getActions(self):\n",
    "        return self.actions\n",
    "    def getRewards(self):\n",
    "        return self.rewards\n",
    "    \n",
    "    def saveState(self,state):\n",
    "        self.states.append(state)\n",
    "    def saveAction(self,action):\n",
    "        self.actions.append(action)\n",
    "    def saveReward(self,reward):\n",
    "        self.rewards.append(reward)\n",
    "    def saveBuffers(self,state,action,reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "\n",
    "class PPO(object):\n",
    "    def __init__(self,stateSize,actionSize,actionRange,criticLR,actorLR,gamma,epsilon,entropyWeight,loadNN,saveDir,loadDir):\n",
    "        self.stateSize = stateSize\n",
    "        self.actionSize = actionSize\n",
    "        self.actionRange = actionRange\n",
    "        self.criticLR = criticLR\n",
    "        self.actorLR = actorLR\n",
    "        self.GAMMA = gamma\n",
    "        self.EPSILON = epsilon\n",
    "        self.saveDir = saveDir\n",
    "        self.entropyWeight = entropyWeight\n",
    "        \n",
    "        if loadNN:\n",
    "            # load Model\n",
    "            self.actor,self.oldactor,self.critic = self.loadNN(loadDir)\n",
    "        else:\n",
    "            # critc NN\n",
    "            self.critic = self.buildCriticNet(self.stateSize,1)\n",
    "            \n",
    "            # actor & oldActor NN\n",
    "            self.oldactor = self.buildActorNet(self.stateSize,self.actionRange)\n",
    "            self.actor = self.buildActorNet(self.stateSize,self.actionRange)\n",
    "\n",
    "    # Build Net\n",
    "    def buildActorNet(self,inputSize,continuousActionRange):\n",
    "        # buildActor NN\n",
    "        stateInput = layers.Input(shape = (inputSize,),name ='stateInput')\n",
    "        \n",
    "        dense1 = layers.Dense(200,activation='relu',name = 'dense1',)(stateInput)\n",
    "        dense2 = layers.Dense(50,activation='relu',name = 'dense2')(dense1)\n",
    "        mu = continuousActionRange * layers.Dense(1,activation='tanh',name = 'muOut')(dense2) # mu，既正态分布mean\n",
    "        sigma = layers.Dense(1,activation='softplus',name = 'sigmaOut')(dense2) #sigma，既正态分布\n",
    "        muSig = layers.concatenate([mu,sigma],name = 'muSigOut')\n",
    "        model = keras.Model(inputs = stateInput,outputs = muSig)\n",
    "        #actorOPT = optimizers.Adam(learning_rate = self.actorLR)\n",
    "        actorOPT = RAdam(self.actorLR)\n",
    "        model.compile(optimizer = actorOPT,loss = self.aLoss())\n",
    "        return model\n",
    "\n",
    "    def buildCriticNet(self,inputSize,outputSize):\n",
    "        # buildCritic NN\n",
    "        stateInput = keras.Input(shape = (inputSize,))\n",
    "        dense1 = layers.Dense(200,activation='relu')(stateInput)\n",
    "        dense2 = layers.Dense(50,activation='relu')(dense1)\n",
    "        output = layers.Dense(outputSize)(dense2)\n",
    "        model = keras.Model(inputs = stateInput,outputs = output)\n",
    "        criticOPT = optimizers.Adam(learning_rate=self.criticLR)\n",
    "        model.compile(optimizer = criticOPT,loss = self.cLoss())\n",
    "        return model\n",
    "    \n",
    "    # loss Function\n",
    "    def cLoss(self):\n",
    "        # Critic Loss\n",
    "        def loss(y_true, y_pred):\n",
    "            # y_true: discountedR\n",
    "            # y_pred: critcV = model.predict(states)\n",
    "            \n",
    "            advantage = y_true - y_pred # TD error\n",
    "            loss = tf.reduce_mean(tf.square(advantage))\n",
    "            return loss\n",
    "        return loss\n",
    "    \n",
    "    def aLoss(self):\n",
    "            \n",
    "        def loss(y_true,y_pred):\n",
    "            # y_true: [[actions,adv,piProb]]\n",
    "            # y_pred: muSigma = self.actor(state)\n",
    "            actions = y_true[:,0] # shape : (length,)\n",
    "            advantage = y_true[:,1] # shape : (length,)\n",
    "            oldpiProb = y_true[:,2] # shape : (length,)\n",
    "            #oldpiProb = tf.reshape(oldpiProb,(tf.size(oldpiProb),1)) # shape : (length,1)\n",
    "            \n",
    "            dist = tfp.distributions.Normal(y_pred[:,0],y_pred[:,1])\n",
    "            piProb = dist.prob(actions)\n",
    "            \n",
    "            ratio = piProb/(oldpiProb+1e-6)\n",
    "            surr = ratio * advantage\n",
    "            clipValue = tf.clip_by_value(ratio,1. - self.EPSILON,1. + self.EPSILON ) * advantage\n",
    "            \n",
    "            entropy = tf.reduce_mean(dist.entropy())\n",
    "            \n",
    "            loss = -tf.reduce_mean(tf.minimum(surr,clipValue)) + self.entropyWeight * entropy\n",
    "            return loss\n",
    "        return loss\n",
    "        \n",
    "    # get Action&V\n",
    "    def chooseAction(self,state):\n",
    "        # let actor choose action,use the normal distribution\n",
    "        # state = np.expand_dims(state,0)\n",
    "        muSigma = self.actor(state) # get mu & sigma\n",
    "        mu = muSigma[0][0]\n",
    "        sigma = muSigma[0][1]\n",
    "        if math.isnan(mu) or math.isnan(sigma):\n",
    "            # check mu or sigma is nan\n",
    "            print(\"mu or sigma is nan\")\n",
    "            time.sleep(100000)\n",
    "        normDist = np.random.normal(loc=mu,scale=sigma) # normalDistribution\n",
    "        action = np.clip(normDist,-self.actionRange,self.actionRange) # 在正态分布中随机get一个action\n",
    "        return action,mu,sigma\n",
    "    \n",
    "    def getCriticV(self,state):\n",
    "        # just get critic's predict value\n",
    "        '''if state.ndim < 2:\n",
    "            state = np.expand_dims(state,0)'''\n",
    "        return self.critic.predict(state)\n",
    "    \n",
    "    # Other\n",
    "    def discountReward(self,nextState,rewards):\n",
    "        # Discount future rewards\n",
    "        nextV = self.getCriticV(nextState)\n",
    "        discountedRewards = []\n",
    "        for r in rewards[::-1]:\n",
    "            nextV = r +self.GAMMA*nextV\n",
    "            discountedRewards.append(nextV)\n",
    "        discountedRewards.reverse() # \\ESREVER/\n",
    "        discountedRewards = np.squeeze(discountedRewards)\n",
    "        discountedRewards = np.expand_dims(discountedRewards,axis=1)\n",
    "        #discountedRewards = np.array(discountedRewards)[:, np.newaxis]\n",
    "        return discountedRewards\n",
    "    \n",
    "    def distProb(self,mu,sig,x):\n",
    "        # 获取在正态分布mu,sig下当取x值时的概率\n",
    "        # return shape : (length,1)\n",
    "        mu = np.reshape(mu,(np.size(mu),))\n",
    "        sig = np.reshape(sig,(np.size(sig),))\n",
    "        x = np.reshape(x,(np.size(x),))\n",
    "        \n",
    "        dist = tfp.distributions.Normal(mu,sig)\n",
    "        prob = dist.prob(x)\n",
    "        prob = np.reshape(prob,(np.size(x),1))\n",
    "        return prob\n",
    "    \n",
    "    # Train Functions\n",
    "    def trainCritcActor(self,states,actions,rewards,nextState,criticEpochs,actorEpochs):\n",
    "        # Train ActorNN and CriticNN\n",
    "        # states: Buffer States\n",
    "        # actions: Buffer Actions\n",
    "        # rewards: Buffer Rewards,not discounted yet\n",
    "        # nextState: next single state\n",
    "        # criticEpochs: just criticNN'Epochs\n",
    "        # acotrEpochs: just acotrNN'Epochs\n",
    "        discountedR = self.discountReward(nextState,rewards)\n",
    "        \n",
    "        criticMeanLoss = self.trainCritic(states,discountedR,criticEpochs)\n",
    "        actorMeanLoss = self.trainActor(states,actions,discountedR,actorEpochs)\n",
    "        print(\"A_Loss:\",actorMeanLoss,\"C_Loss:\",criticMeanLoss)\n",
    "        return actorMeanLoss,criticMeanLoss\n",
    "    \n",
    "    def trainCritic(self,states,discountedR,epochs):\n",
    "        # Trian Critic \n",
    "        # states: Buffer States\n",
    "        # discountedR: Discounted Rewards\n",
    "        # Epochs: just Epochs\n",
    "        \n",
    "        # IDK why this should be list...It just work...\n",
    "        # If discountR in np.array type it will throw 'Failed to find data adapter that can handle'        \n",
    "        discountedR = discountedR.tolist()\n",
    "        his = self.critic.fit(x = states,y = discountedR,epochs=epochs,verbose = 0)\n",
    "        return np.mean(his.history['loss'])\n",
    "    \n",
    "    def trainActor(self,states,actions,discountedR,epochs):\n",
    "        # Trian Actor\n",
    "        # states: Buffer States\n",
    "        # actions: Buffer Actions\n",
    "        # discountedR: Discounted Rewards\n",
    "        # Epochs: just Epochs\n",
    "        states = np.asarray(states)\n",
    "        actions = np.asarray(actions,dtype=np.float32)\n",
    "        \n",
    "        # predict Musig with old Actor NN\n",
    "        oldPiMuSig = self.oldactor.predict(states)\n",
    "        oldPiProb = self.distProb(oldPiMuSig[:,0],oldPiMuSig[:,1],actions)\n",
    "\n",
    "        criticV = self.critic.predict(states)\n",
    "        advantage = copy.deepcopy(discountedR - criticV)\n",
    "        \n",
    "        # pack [[actions,advantage,oldPiProb]] as y_true\n",
    "        y_true = np.append(actions,advantage,axis=1)\n",
    "        y_true = np.append(y_true,oldPiProb,axis = 1)\n",
    "\n",
    "        # train start\n",
    "        his = self.actor.fit(x = states,y = y_true,epochs = epochs,verbose = 0)\n",
    "        return np.mean(his.history['loss'])\n",
    "    \n",
    "    def updateOldActor(self):\n",
    "        # updateOldActorNN\n",
    "        self.oldactor.set_weights(self.actor.get_weights())\n",
    "    \n",
    "    # save&load\n",
    "    def saveNN(self,score):\n",
    "        score = \"_\" + str(round(score))\n",
    "        actor_save_dir = self.saveDir+datetime.datetime.now().strftime(\"%H%M%S\")+score+\"/actor.h5\"\n",
    "        critic_save_dir = self.saveDir+datetime.datetime.now().strftime(\"%H%M%S\")+score+\"/critic.h5\"\n",
    "        self.actor.save(actor_save_dir)\n",
    "        self.critic.save(critic_save_dir)\n",
    "        print(\"Model Saved\")\n",
    "    \n",
    "    def loadNN(self,loadDir):\n",
    "        actorDir = loadDir+\"/actor.h5\"\n",
    "        criticDir = loadDir+\"/critic.h5\"\n",
    "        actor_net_loaded = tf.keras.models.load_model(actorDir)\n",
    "        old_Actor_net_loaded = tf.keras.models.load_model(actorDir)\n",
    "        critic_net_loaded = tf.keras.models.load_model(criticDir)\n",
    "        \n",
    "        print(\"Model Loaded\")\n",
    "        return actor_net_loaded,old_Actor_net_loaded,critic_net_loaded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDir = \"PPO-Model/\"+datetime.datetime.now().strftime(\"%m%d-%H%M\")+\"/\"\n",
    "allEpRewardsHis = []\n",
    "allEpSigmaHis = []\n",
    "agent = PPO(STATE_SIZE,ACTION_SIZE,CTN_ACTION_RANGE,CRITIC_LR,ACTOR_LR,GAMMA,EPSILON,ENTROPY_WHEIGHT,loadNN=False,saveDir=modelDir,loadDir = \"NAH\")\n",
    "bestScore = 200.\n",
    "stopTrainCounter = 0\n",
    "\n",
    "TotalRewardHis = []\n",
    "TotalSigmaHis = []\n",
    "TotalActorLossHis = []\n",
    "TotalCriticLossHis = []\n",
    "epHis = []\n",
    "for ep in range(MAX_EP):\n",
    "    stopTrainCounter -= 1\n",
    "    epHis.append(ep)\n",
    "    s = env.reset()\n",
    "    s = s.tolist()\n",
    "    epBuffer = buffer()\n",
    "    epTotalReward = 0\n",
    "    \n",
    "    # Historys\n",
    "    epStepHis = []\n",
    "    epRewardHis = []\n",
    "    epSigmaHis = []\n",
    "    epActorLossHis = []\n",
    "    epCriticLossHis = []\n",
    "    for t in range(EP_LENGTH):\n",
    "        env.render()\n",
    "        if t%ACTION_INTERVAL ==0: # take action every ACTION_INTERVAL steps\n",
    "            a,mu,sigma = agent.chooseAction(np.expand_dims(s,0))\n",
    "            a = [a]\n",
    "            \n",
    "            nextS,r,done,_ = env.step(a) # Take Action\n",
    "            nextS = np.squeeze(nextS).tolist()\n",
    "            r = (r+8)/8 #normalize reward\n",
    "            epTotalReward +=r\n",
    "\n",
    "            # Save Buffers & History\n",
    "            epBuffer.saveBuffers(s,a,r)\n",
    "            epRewardHis.append(r)\n",
    "            epStepHis.append(t)\n",
    "            epSigmaHis.append(sigma)\n",
    "        else: # take 0 action between ACTION_INTERVAL steps\n",
    "            nextS,r,done,_ = env.step([0])\n",
    "            nextS = np.squeeze(nextS).tolist()\n",
    "            r = (r+8)/8 #normalize reward\n",
    "            epTotalReward +=r\n",
    "\n",
    "        s = nextS\n",
    "        \n",
    "        # time to update PPO!\n",
    "        if (t+1)% BATCH*2 == 0 or t == EP_LENGTH-1:\n",
    "            bs = epBuffer.getStates()\n",
    "            ba = epBuffer.getActions()\n",
    "            br = epBuffer.getRewards()\n",
    "            epBuffer.clearBuffer()\n",
    "            agent.updateOldActor()\n",
    "            if TRAIN:\n",
    "                epActorLoss, epCriticLoss= agent.trainCritcActor(bs,ba,br,np.expand_dims(s,0),CRITIC_EPOCH,ACTOR_EPOCH)\n",
    "                epActorLossHis.append(epActorLoss)\n",
    "                epCriticLossHis.append(epCriticLoss)\n",
    "    \n",
    "    # save Historys\n",
    "    TotalActorLossHis.append(np.mean(epActorLossHis))\n",
    "    TotalCriticLossHis.append(np.mean(epCriticLossHis))\n",
    "    if ep == 0:\n",
    "        allEpRewardsHis.append(epTotalReward)\n",
    "    else:\n",
    "        \n",
    "        allEpRewardsHis.append(epTotalReward)\n",
    "        #allEpRewards.append(allEpRewards[-1]*0.9+epTotalReward*0.1)\n",
    "    TotalRewardHis.append(epTotalReward)\n",
    "    TotalSigmaHis.append(np.mean(epSigmaHis))\n",
    "    \n",
    "    clear_output()\n",
    "    print('Ep %i Over:' % ep,'TotalReward:%i' % epTotalReward)\n",
    "    \n",
    "    # figure\n",
    "    plt.figure(figsize=(21,13))\n",
    "    plt.subplot(3,2,1)\n",
    "    plt.plot(epStepHis,epRewardHis)\n",
    "    plt.title(\"this ep reward his\")\n",
    "    plt.subplot(3,2,2)\n",
    "    plt.scatter(epHis,TotalRewardHis)\n",
    "    plt.title(\"total epReward his\")\n",
    "    plt.subplot(3,2,3)\n",
    "    plt.plot(epStepHis,epSigmaHis)\n",
    "    plt.title(\"this ep Sigma his\")\n",
    "    plt.subplot(3,2,4)\n",
    "    plt.plot(epHis,TotalSigmaHis)\n",
    "    plt.title(\"Average Sigma his\")\n",
    "    plt.subplot(3,2,5)\n",
    "    plt.plot(epHis,TotalActorLossHis)\n",
    "    plt.title(\"Average ActorLoss his\")\n",
    "    plt.subplot(3,2,6)\n",
    "    plt.plot(epHis,TotalCriticLossHis)\n",
    "    plt.title(\"Average CriticLoss his\")\n",
    "    # whent get a new record\n",
    "    if (bestScore < epTotalReward):\n",
    "        figDir = modelDir+datetime.datetime.now().strftime(\"%H%M%S\")+\"_\"+str(round(epTotalReward))+\".png\"\n",
    "        bestScore = epTotalReward\n",
    "        # save NN & pic\n",
    "        agent.saveNN(epTotalReward)\n",
    "        plt.savefig(figDir)\n",
    "        if ep != 0:\n",
    "            TRAIN = False\n",
    "            stopTrainCounter = 5\n",
    "    plt.show()\n",
    "    if stopTrainCounter <= 0:\n",
    "        TRAIN = True"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86e2db13b09bd6be22cb599ea60c1572b9ef36ebeaa27a4c8e961d6df315ac32"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
